{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format for ML algorithms : \n",
    "- Data must be in tabular form\n",
    "- Training features must be numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words Model\n",
    "- extract word tokens\n",
    "- compute frequency of word tokens\n",
    "- construct a word vector out of these frequencies and vocabulary of corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the scikit-learn `CountVectorizer` which takes a collection of text documents and creates a matrix of token counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A corpus of sentences.\n",
    "corpus = [\n",
    "  \"Red Bull drops hint on F1 engine.\",\n",
    "  \"Honda exits F1, leaving F1 partner Red Bull.\",\n",
    "  \"Hamilton eyes record eighth F1 title.\",\n",
    "  \"Aston Martin announces sponsor.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain frequency BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition: \n",
    "\n",
    "First, we have **vocabulary** list that consists of all unique words in documents. \n",
    "\n",
    "Next, convert the doc into a BOW **vector**. \n",
    "\n",
    "Vocabularyâ†’ a,an,decade,endangered,have,is,jungle,king,lifespans,lion,Lions,of,species,the,The\n",
    "\n",
    "eg. corpus = \"The lion is the king of the jungle\"\n",
    "\n",
    "vector = [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *fit_transform* method does two things:\n",
    "1. It learns a vocabulary dictionary from the corpus.\n",
    "2. It returns a matrix where each row represents a document and each column represents a token (i.e. term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['announces' 'aston' 'bull' 'drops' 'eighth' 'engine' 'exits' 'eyes' 'f1'\n",
      " 'hamilton' 'hint' 'honda' 'leaving' 'martin' 'on' 'partner' 'record'\n",
      " 'red' 'sponsor' 'title']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'red': 17,\n",
       " 'bull': 2,\n",
       " 'drops': 3,\n",
       " 'hint': 10,\n",
       " 'on': 14,\n",
       " 'f1': 8,\n",
       " 'engine': 5,\n",
       " 'honda': 11,\n",
       " 'exits': 6,\n",
       " 'leaving': 12,\n",
       " 'partner': 15,\n",
       " 'hamilton': 9,\n",
       " 'eyes': 7,\n",
       " 'record': 16,\n",
       " 'eighth': 4,\n",
       " 'title': 19,\n",
       " 'aston': 1,\n",
       " 'martin': 13,\n",
       " 'announces': 0,\n",
       " 'sponsor': 18}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View list of features (tokens).\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# View vocabulary dictionary.\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, the `CountVectorizer` generates a sparse matrix. The sparse matrix object includes a number of useful methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 8)\t2\n",
      "  (1, 11)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 15)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 19)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 13)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 18)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. column 1: a list of tuples. \n",
    "- 1st value: represents docs. There are 4 rows = 4 docs. 1=row1, 2=row2,...\n",
    "- 2nd value: token in index in the vocabulary list. \n",
    "2. Column 2: token counts in a row. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary BOW with custom tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, we can customize our tokenizer (i.e. write our own method to return the desired tokens). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, we start by importing spaCy and loading a statistical model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a tokenizer callback using spaCy under the hood. Here, we tokenize\n",
    "# the passed-in text and return the tokens, filtering out punctuation.\n",
    "def spacy_tokenizer(doc):\n",
    "    return [t.text for t in nlp(doc) if not t.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True)\n",
    "bow = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aston' 'Bull' 'F1' 'Hamilton' 'Honda' 'Martin' 'Red' 'announces' 'drops'\n",
      " 'eighth' 'engine' 'exits' 'eyes' 'hint' 'leaving' 'on' 'partner' 'record'\n",
      " 'sponsor' 'title']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Red': 6,\n",
       " 'Bull': 1,\n",
       " 'drops': 8,\n",
       " 'hint': 13,\n",
       " 'on': 15,\n",
       " 'F1': 2,\n",
       " 'engine': 10,\n",
       " 'Honda': 4,\n",
       " 'exits': 11,\n",
       " 'leaving': 14,\n",
       " 'partner': 16,\n",
       " 'Hamilton': 3,\n",
       " 'eyes': 12,\n",
       " 'record': 17,\n",
       " 'eighth': 9,\n",
       " 'title': 19,\n",
       " 'Aston': 0,\n",
       " 'Martin': 5,\n",
       " 'announces': 7,\n",
       " 'sponsor': 18}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-array method\n",
    "View the 2d matrix of our tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0]\n",
      " [0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0]\n",
      " [0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1]\n",
      " [1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0]]\n",
      "\n",
      "Indexing and slicing.\n",
      "  (0, 6)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 10)\t1\n",
      "\n",
      "  (0, 6)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 10)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 16)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bow.toarray())\n",
    "print()\n",
    "print('Indexing and slicing.')\n",
    "print(bow[0])\n",
    "print()\n",
    "print(bow[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Scores \n",
    "1. Dot Product\n",
    "2. Magnitude of a vector\n",
    "3. Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product of two vectors $\\vec{V}$ and $\\vec{W}$ is given by:\n",
    "\n",
    "$$\n",
    "\\vec{V} = (v_1, v_2, \\ldots, v_n), \\quad \\vec{W} = (w_1, w_2, \\ldots, w_n)\n",
    "$$\n",
    "\n",
    "The dot product $\\vec{V}$ $\\cdot$ $\\vec{W}$ is calculated as:\n",
    "\n",
    "$$\n",
    "\\vec{V} \\cdot \\vec{W} = (v_1 \\times w_1) + (v_2 \\times w_2) + \\ldots + (v_n \\times w_n)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any vector $\\vec{V} = (v_1, v_2, \\ldots, v_n)$, the magnitude is defined as:\n",
    "\n",
    "$$\n",
    "\\|\\vec{V}\\| = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity between vectors $\\vec{V}$ and $\\vec{W}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{cos}(\\vec{V}, \\vec{W}) = \\frac{\\vec{V} \\cdot \\vec{W}}{\\|\\vec{V}\\| \\cdot \\|\\vec{W}\\|}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is used to find the angle between those 2 vectors. If the value of cosine is 1, the vectors point in the same direction. Otherwise, if it is 0, the vectors are orthogonal (dissimilar). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Red Bull drops hint on F1 engine.', 'Honda exits F1, leaving F1 partner Red Bull.', 'Hamilton eyes record eighth F1 title.', 'Aston Martin announces sponsor.']\n",
      "Doc 1 vs Doc 2: 0.4285714285714286\n",
      "Doc 1 vs Doc 3: 0.15430334996209194\n",
      "Doc 1 vs Doc 4: 0.0\n"
     ]
    }
   ],
   "source": [
    "# The cosine method expects array_like inputs, so we need to generate\n",
    "# arrays from our sparse matrix.\n",
    "doc1_vs_doc2 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[1].toarray()[0])\n",
    "doc1_vs_doc3 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[2].toarray()[0])\n",
    "doc1_vs_doc4 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[3].toarray()[0])\n",
    "\n",
    "print(corpus)\n",
    "\n",
    "print(f\"Doc 1 vs Doc 2: {doc1_vs_doc2}\")\n",
    "print(f\"Doc 1 vs Doc 3: {doc1_vs_doc3}\")\n",
    "print(f\"Doc 1 vs Doc 4: {doc1_vs_doc4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine_similarity from `scikit-learn` lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.42857143 0.15430335 0.        ]\n",
      " [0.42857143 1.         0.15430335 0.        ]\n",
      " [0.15430335 0.15430335 1.         0.        ]\n",
      " [0.         0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# cosine_similarity can take either array-likes or sparse matrices.\n",
    "print(cosine_similarity(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram models\n",
    "Contigous sequence of n elements (or words) in a given document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW might have shortcomings: \n",
    "\n",
    "Context of the words is lost as we might get identical vectors if there are 2 docs with exact the same words. Let's see an example: \n",
    "\n",
    "'The movie was good and not boring' -> Positive \n",
    "\n",
    "'The movie was not good and boring' -> Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For (n = 1) (Bag-of-Words):\n",
    "\n",
    "Sentence: \"for you a thousand times over\"\n",
    "\n",
    "For (n = 2) (bigrams):\n",
    "\n",
    "n-grams: ['for you', 'you a', 'a thousand', 'thousand times', 'times over']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cons: Curse of dimensionality and higher order n-grams are rare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aston' 'Aston Martin' 'Bull' 'Bull drops' 'F1' 'F1 engine' 'F1 leaving'\n",
      " 'F1 partner' 'F1 title' 'Hamilton' 'Hamilton eyes' 'Honda' 'Honda exits'\n",
      " 'Martin' 'Martin announces' 'Red' 'Red Bull' 'announces'\n",
      " 'announces sponsor' 'drops' 'drops hint' 'eighth' 'eighth F1' 'engine'\n",
      " 'exits' 'exits F1' 'eyes' 'eyes record' 'hint' 'hint on' 'leaving'\n",
      " 'leaving F1' 'on' 'on F1' 'partner' 'partner Red' 'record'\n",
      " 'record eighth' 'sponsor' 'title']\n",
      "Number of features: 40\n"
     ]
    }
   ],
   "source": [
    "# Setting ngram_range parameter to (1, 2) generates both unigrams and bigrams.\n",
    "vectorizer = CountVectorizer(lowercase=False, binary=True, ngram_range=(1,2))\n",
    "bigrams = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('Number of features: {}'.format(len(vectorizer.get_feature_names_out())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aston Martin' 'Bull drops' 'F1 engine' 'F1 leaving' 'F1 partner'\n",
      " 'F1 title' 'Hamilton eyes' 'Honda exits' 'Martin announces' 'Red Bull'\n",
      " 'announces sponsor' 'drops hint' 'eighth F1' 'exits F1' 'eyes record'\n",
      " 'hint on' 'leaving F1' 'on F1' 'partner Red' 'record eighth']\n",
      "{'Red Bull': 9, 'Bull drops': 1, 'drops hint': 11, 'hint on': 15, 'on F1': 17, 'F1 engine': 2, 'Honda exits': 7, 'exits F1': 13, 'F1 leaving': 3, 'leaving F1': 16, 'F1 partner': 4, 'partner Red': 18, 'Hamilton eyes': 6, 'eyes record': 14, 'record eighth': 19, 'eighth F1': 12, 'F1 title': 5, 'Aston Martin': 0, 'Martin announces': 8, 'announces sponsor': 10}\n"
     ]
    }
   ],
   "source": [
    "# Setting n_gram range to (2, 2) generates only bigrams.\n",
    "vectorizer = CountVectorizer(lowercase=False, binary=True, ngram_range=(2,2))\n",
    "bigrams = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE 1: Create a spacy_tokenizer callback which takes a string and returns a list of tokens (each token's text) with punctuation filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aerial' 'Assad' 'Bashar' 'CMOS' 'Canon' 'During' 'GPS' 'President'\n",
      " 'SLRs' 'Students' 'Syria' 'Syrian' 'Teenagers' 'US' 'a' 'about' 'aerial'\n",
      " 'against' 'al' 'are' 'as' 'birdview' 'cellphones' 'contours' 'danger'\n",
      " 'days' 'digital' 'early' 'enabled' 'enthusiastic' 'features' 'find'\n",
      " 'from' 'great' 'ground' 'heaps' 'identify' 'if' 'image' 'in' 'is' 'it'\n",
      " 'lake' 'land' 'leader' 'level' 'much' 'neighbourhood' 'nâ€™t' 'of' 'or'\n",
      " 'order' 'paths' 'pay' 'photograph' 'photographs' 'photography' 'points'\n",
      " 'pretty' 'price' 'river' 'rubbish' 'sensor' 'specific' 'strikes' 'study'\n",
      " 'such' 'take' 'taking' 'technology' 'tells' 'terrestrial' 'that' 'the'\n",
      " 'their' 'to' 'undisputed' 'use' 'visible' 'was' 'way' 'will']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Students': 9,\n",
       " 'use': 77,\n",
       " 'their': 74,\n",
       " 'GPS': 6,\n",
       " 'enabled': 28,\n",
       " 'cellphones': 22,\n",
       " 'to': 75,\n",
       " 'take': 67,\n",
       " 'birdview': 21,\n",
       " 'photographs': 55,\n",
       " 'of': 49,\n",
       " 'a': 14,\n",
       " 'land': 43,\n",
       " 'in': 39,\n",
       " 'order': 51,\n",
       " 'find': 31,\n",
       " 'specific': 63,\n",
       " 'danger': 24,\n",
       " 'points': 57,\n",
       " 'such': 66,\n",
       " 'as': 20,\n",
       " 'rubbish': 61,\n",
       " 'heaps': 35,\n",
       " 'Teenagers': 12,\n",
       " 'are': 19,\n",
       " 'enthusiastic': 29,\n",
       " 'about': 15,\n",
       " 'taking': 68,\n",
       " 'aerial': 16,\n",
       " 'photograph': 54,\n",
       " 'study': 65,\n",
       " 'neighbourhood': 47,\n",
       " 'Aerial': 0,\n",
       " 'photography': 56,\n",
       " 'is': 40,\n",
       " 'great': 33,\n",
       " 'way': 80,\n",
       " 'identify': 36,\n",
       " 'terrestrial': 71,\n",
       " 'features': 30,\n",
       " 'that': 72,\n",
       " 'nâ€™t': 48,\n",
       " 'visible': 78,\n",
       " 'from': 32,\n",
       " 'the': 73,\n",
       " 'ground': 34,\n",
       " 'level': 45,\n",
       " 'lake': 42,\n",
       " 'contours': 23,\n",
       " 'or': 50,\n",
       " 'river': 60,\n",
       " 'paths': 52,\n",
       " 'During': 5,\n",
       " 'early': 27,\n",
       " 'days': 25,\n",
       " 'digital': 26,\n",
       " 'SLRs': 8,\n",
       " 'Canon': 4,\n",
       " 'was': 79,\n",
       " 'pretty': 58,\n",
       " 'much': 46,\n",
       " 'undisputed': 76,\n",
       " 'leader': 44,\n",
       " 'CMOS': 3,\n",
       " 'image': 38,\n",
       " 'sensor': 62,\n",
       " 'technology': 69,\n",
       " 'Syrian': 11,\n",
       " 'President': 7,\n",
       " 'Bashar': 2,\n",
       " 'al': 18,\n",
       " 'Assad': 1,\n",
       " 'tells': 70,\n",
       " 'US': 13,\n",
       " 'it': 41,\n",
       " 'will': 81,\n",
       " 'pay': 53,\n",
       " 'price': 59,\n",
       " 'if': 37,\n",
       " 'strikes': 64,\n",
       " 'against': 17,\n",
       " 'Syria': 10}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 5 docs (=rows) in this new corpus. \n",
    "corpus = [\n",
    "    \"Students use their GPS-enabled cellphones to take birdview photographs of a land in order to find specific danger points such as rubbish heaps.\",\n",
    "    \"Teenagers are enthusiastic about taking aerial photograph in order to study their neighbourhood.\",\n",
    "    \"Aerial photography is a great way to identify terrestrial features that arenâ€™t visible from the ground level, such as lake contours or river paths.\",\n",
    "    \"During the early days of digital SLRs, Canon was pretty much the undisputed leader in CMOS image sensor technology.\",\n",
    "    \"Syrian President Bashar al-Assad tells the US it will 'pay the price' if it strikes against Syria.\"\n",
    "]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_tokenizer(doc):\n",
    "    return (t.text for t in nlp(doc) if not t.is_punct)\n",
    "\n",
    "# Initialize a CountVectorizer object and set it to use\n",
    "# your spacy_tokenizer with lower-casing off and to create a binary BOW.\n",
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True)\n",
    "binary_bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string below is a whole paragraph. We want to create another\n",
    "binary BOW but using the vocabulary of our *current* CountVectorizer. This means\n",
    "that words in this paragraph which AREN'T already in the vocabulary won't be\n",
    "epresented. This is to illustrate how BOW can't handle out-of-vocabulary words\n",
    "unless you rebuild your whole vocabulary. Still, we'll see that if there's\n",
    "enough overlapping vocabulary, some similarity can still be picked up.\n",
    "\n",
    "Note that we call 'transform' only instead of 'fit_transform' because the fit step (i.e. vocabulary build) is already done and we don't want to re-fit here.\n",
    "\n",
    "EXERCISE 2: using the pairwise cosine_similarity method from sklearn,\n",
    "calculate the similarities between each document from the corpus against\n",
    "this new document (new_bow). HINT: You can pass two parameters to\n",
    "cosine_similarity in this case. \n",
    "\n",
    "Which document is the most similar? Which is the least similar? Do the results make sense based on what you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69565217],\n",
       "       [0.40482045],\n",
       "       [0.29192018],\n",
       "       [0.19658927],\n",
       "       [0.0521286 ]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example new document\n",
    "s = [\"Teenagers take aerial shots of their neighbourhood using digital cameras sitting in old bottles which are launched via kites - a common toy for children living in the favelas. They then use GPS-enabled smartphones to take pictures of specific danger points - such as rubbish heaps, which can become a breeding ground for mosquitoes carrying dengue fever.\"]\n",
    "\n",
    "new_bow = vectorizer.transform(s)\n",
    "\n",
    "# Calculate cosine similarities with all documents in the corpus\n",
    "similarities = cosine_similarity(binary_bow, new_bow)\n",
    "\n",
    "similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: \n",
    "1. 5 values - correspond to 5 docs in the 'corpus'. \n",
    "2. The 1st row has the highest consie similarity score = 0.69 => row 1 has the most similar tokens to the new paragraph. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
