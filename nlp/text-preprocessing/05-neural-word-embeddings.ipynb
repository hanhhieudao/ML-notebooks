{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Word Embeddings\n",
    "\n",
    "Word embeddings represent words so that similar meanings have similar vectors. Neural embeddings serve three main purposes:\n",
    "\n",
    "- Finding nearest neighbors for recommendations or clustering.\n",
    "- Serving as input for supervised machine learning tasks.\n",
    "- Visualizing concepts and relationships between categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents vs. Words\n",
    "- NWE convert single words into vectors. \n",
    "- A doc becomes a **sequence of vectors**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec\n",
    "\n",
    "Train a neural network to make i binary prediction about whether an input word and some output word can be found close together in the text corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe \n",
    "\n",
    "- Not use neural network\n",
    "- Work like a recommendation system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip /Users/hanhhieudao/Downloads/ML-notebooks/nlp/data/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is an open-source Python library designed for natural language processing (NLP) and topic modeling. It is particularly well-known for its efficient implementations of various word embedding algorithms and topic modeling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format(\n",
    "    '../data/GoogleNews-vectors-negative300.bin',\n",
    "    binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogies \n",
    "\n",
    "1. Doing arithmetic on vectors (+ and -)\n",
    "2. Eg. \n",
    "- in math: vec(\"king\") - vec(\"man\") â‰ˆ vec(\"queen\") - vec(\"woman\")\n",
    "- in code: x = King - Man + Woman \n",
    "\n",
    "Find the closest word vector to x (the resule would be Queen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to find the analogies\n",
    "def find_analogies(w1, w2, w3):\n",
    "    # w1 - w2 = ? - w3\n",
    "    # e.g. king - man = ? - woman\n",
    "    #      ? = +king +woman -man\n",
    "    r = word_vectors.most_similar(positive=[w1, w3], negative=[w2])\n",
    "    print(\"%s - %s = %s - %s\" % (w1, w2, r[0][0], w3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man = queen - woman\n"
     ]
    }
   ],
   "source": [
    "find_analogies('king', 'man', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "france - paris = england - london\n"
     ]
    }
   ],
   "source": [
    "find_analogies('france', 'paris', 'london')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674735069275),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411403656006)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's have a closer look at r\n",
    "r = word_vectors.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r is a list of tuples where each tuple contains a word and its similarity score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, write a function to find the most similar word \n",
    "def nearest_neighbors(w):\n",
    "    r = word_vectors.most_similar(positive=[w])\n",
    "    print(\"neighbors of: %s\" % w)\n",
    "    for word, score in r:\n",
    "        print(\"\\t%s\" % word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbors of: king\n",
      "\tkings\n",
      "\tqueen\n",
      "\tmonarch\n",
      "\tcrown_prince\n",
      "\tprince\n",
      "\tsultan\n",
      "\truler\n",
      "\tprinces\n",
      "\tPrince_Paras\n",
      "\tthrone\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors('king')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
